# Qwen2.5-VL LoRA微调配置

# 模型配置
model:
  name: "Qwen/Qwen2.5-VL-3B-Instruct"
  max_length: 2048
  image_size: 224

# 数据配置
data:
  train_path: "data/train.json"
  val_path: "data/val.json"
  batch_size: 2
  num_workers: 4

# 训练配置
training:
  num_epochs: 3
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  gradient_accumulation_steps: 4

# LoRA配置
lora:
  r: 16
  alpha: 32
  dropout: 0.1
  target_modules: "q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj"
  use_rslora: false

# 优化配置
optimization:
  use_fp16: true
  mixed_precision: "fp16"
  load_in_8bit: false
  load_in_4bit: false
  use_device_map: true

# 输出配置
output:
  dir: "output"
  save_every: 1

# 日志配置
logging:
  use_wandb: false
  wandb_project: "qwen2.5vl-lora"
  wandb_run_name: null 